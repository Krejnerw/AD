{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3fb0435c4fc0f4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d88e1b2f613dcb1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/01 15:25:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/01 15:25:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ce3216d7658c:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Apache SQL and Hive</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Apache SQL and Hive>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ścieżka do bazy danych hurtowni danych oraz plików\n",
    "# należy dostosować do ścieżki względnej, w której umieszczony został bieżący notebook\n",
    "warehouse_location = '/opt/spark/work-dir/L7/db/metastore_db'\n",
    "\n",
    "# utworzenie sesji Spark, ze wskazaniem włączenia obsługi Hive oraz\n",
    "# lokalizacją przechowywania hurtowni danych\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Apache SQL and Hive\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location)\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6819754889e199",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Zadanie 1 \n",
    "Pamiętacie plik zamówienia.txt ? Plik został umieszczony w folderze z labem w repozytorium.\n",
    "Wczytaj ten plik za pomocą Sparka do dowolnego typu danych (RDD, Spark DataFrame) i dokonaj transformacji tak aby:\n",
    "- naprawić problemy z kodowaniem znaków (replace?) w kolumnie Sprzedawca\n",
    "- poprawić format danych w kolumnie Utarg\n",
    "- dodać odpowiednie typy danych\n",
    "- kolumna idZamowienia powinna być traktowana jako klucz (indeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6e0ed1b0cba2ba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Kraj='Polska', Sprzedawca='Kowalski', Data zamowienia='16.07.2003', idZamowienia=10248, Utarg='440,00 z\\x88')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, to_date\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType, StructField, StructType, DateType\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Kraj\", StringType(), True),\n",
    "    StructField(\"Sprzedawca\", StringType(), True),\n",
    "    StructField(\"Data zamowienia\", StringType(), True),\n",
    "    StructField(\"idZamowienia\", IntegerType(), True),\n",
    "    StructField(\"Utarg\", StringType(), True)\n",
    "])\n",
    "df = spark.read.csv('zamowienia.txt', sep=\";\", header=True, schema=schema)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123c939cc04fa01b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------------+-----+-------+\n",
      "|Kraj  |Sprzedawca|Data zamowienia|id   |Utarg  |\n",
      "+------+----------+---------------+-----+-------+\n",
      "|Polska|Kowalski  |2003-07-16     |10248|440.0  |\n",
      "|Polska|Sowiaski  |2003-07-10     |10249|1863.4 |\n",
      "|Niemcy|Peacock   |2003-07-12     |10250|1552.6 |\n",
      "|Niemcy|Leverling |2003-07-15     |10251|654.06 |\n",
      "|Niemcy|Peacock   |2003-07-11     |10252|3597.9 |\n",
      "|Niemcy|Leverling |2003-07-16     |10253|1444.8 |\n",
      "|Polska|Kowalski  |2003-07-23     |10254|556.62 |\n",
      "|Polska|Dudek     |2003-07-15     |10255|2490.5 |\n",
      "|Niemcy|Leverling |2003-07-17     |10256|517.8  |\n",
      "|Niemcy|Peacock   |2003-07-22     |10257|1119.9 |\n",
      "|Niemcy|Davolio   |2003-07-23     |10258|1614.88|\n",
      "|Niemcy|Peacock   |2003-07-25     |10259|100.8  |\n",
      "|Niemcy|Peacock   |2003-07-29     |10260|1504.65|\n",
      "|Niemcy|Peacock   |2003-07-30     |10261|448.0  |\n",
      "|Niemcy|Callahan  |2003-07-25     |10262|584.0  |\n",
      "|Polska|Dudek     |2003-07-31     |10263|1873.8 |\n",
      "|Polska|Sowiaski  |2003-08-23     |10264|695.62 |\n",
      "|Niemcy|Fuller    |2003-08-12     |10265|1176.0 |\n",
      "|Niemcy|Leverling |2003-07-31     |10266|346.56 |\n",
      "|Niemcy|Peacock   |2003-08-06     |10267|3536.6 |\n",
      "+------+----------+---------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    if input_str is None:\n",
    "        return None\n",
    "    # Zamiana znaków diakrytycznych na ich podstawowe odpowiedniki\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "    \n",
    "# zdefiniowanie udf do utworzenia wlasnej funkcji, ktora bedzie uzyta do przetworzenia danych \n",
    "remove_accents_udf = udf(remove_accents, StringType())\n",
    "df = df.withColumn(\"Sprzedawca\", remove_accents_udf(col(\"Sprzedawca\")))\n",
    "\n",
    "df = df.withColumn(\"Data zamowienia\", to_date(col(\"Data zamowienia\"), \"dd.MM.yyyy\"))\n",
    "df = df.withColumn(\"Utarg\", regexp_replace(col(\"Utarg\"), \"[^0-9,]\", \"\"))\n",
    "df = df.withColumn(\"Utarg\", regexp_replace(col(\"Utarg\"), \",\", \".\"))\n",
    "df = df.withColumn(\"Utarg\", col(\"Utarg\").cast(FloatType()))\n",
    "\n",
    "df = df.withColumnRenamed(\"idZamowienia\", \"id\")\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05cd7ce8-e0ef-4c6b-9f89-9580c5155661",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Kraj: string (nullable = true)\n",
      " |-- Sprzedawca: string (nullable = true)\n",
      " |-- Data zamowienia: date (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Utarg: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5411070a7ab073",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Zadanie 2 \n",
    "Po wykonaniu zadania 1, wykorzystaj przykłady z laboratorium i:\n",
    "2.1 wykonaj wiaderkowanie danych i wykonaj dowolne zapytanie agregujące na tych danych vs. dane nie podzielone na wiaderka - porównaj czas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df954d6a8e147f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"ZAMOWIENIA_DATA\")\n",
    "\n",
    "buckets = spark.sql(\"select distinct Sprzedawca from ZAMOWIENIA_DATA\").count()\n",
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34921991de31fd4f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/01 15:25:53 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/12/01 15:25:53 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/12/01 15:25:59 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/12/01 15:25:59 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.2\n",
      "24/12/01 15:26:00 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "                                                                                24/12/01 15:26:03 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/12/01 15:26:03 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/12/01 15:26:03 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/12/01 15:26:03 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+------------------+\n",
      "|Sprzedawca|count(Sprzedawca)|        avg(Utarg)|\n",
      "+----------+-----------------+------------------+\n",
      "|   Davolio|              117| 1559.829831897703|\n",
      "|  Callahan|               99|1242.7542393616957|\n",
      "| Leverling|              125|1609.5701606445311|\n",
      "|  Sowiaski|               65|1115.8096831688515|\n",
      "|  Kowalski|               42| 1637.910719916934|\n",
      "|   Peacock|              151|1495.1237175417261|\n",
      "|      King|               67|1745.7162600844654|\n",
      "|    Fuller|               92| 1766.345436718153|\n",
      "|     Dudek|               41|1830.4399899738592|\n",
      "+----------+-----------------+------------------+\n",
      "\n",
      "bucketed end: 0:00:01.058409\n",
      "+----------+-----------------+------------------+\n",
      "|Sprzedawca|count(Sprzedawca)|        avg(Utarg)|\n",
      "+----------+-----------------+------------------+\n",
      "|  Sowiaski|               65|1115.8096831688515|\n",
      "|   Peacock|              151|1495.1237175417261|\n",
      "|      King|               67|1745.7162600844654|\n",
      "|     Dudek|               41|1830.4399899738592|\n",
      "|   Davolio|              117| 1559.829831897703|\n",
      "|    Fuller|               92| 1766.345436718153|\n",
      "| Leverling|              125|1609.5701606445311|\n",
      "|  Kowalski|               42| 1637.910719916934|\n",
      "|  Callahan|               99|1242.7542393616957|\n",
      "+----------+-----------------+------------------+\n",
      "\n",
      "not bucketed end: 0:00:00.350837\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "data = df.write.bucketBy(buckets, 'Sprzedawca').mode('overwrite').sortBy('Utarg').saveAsTable('zamowienia_sprzedawca_bucketed')\n",
    "\n",
    "start = datetime.now()\n",
    "spark.sql(\"select Sprzedawca, count(Sprzedawca), avg(Utarg) from zamowienia_sprzedawca_bucketed group by Sprzedawca\").show()\n",
    "print(f'bucketed end: {datetime.now()-start}')\n",
    "\n",
    "start = datetime.now()\n",
    "spark.sql(\"select Sprzedawca, count(Sprzedawca), avg(Utarg) from ZAMOWIENIA_DATA group by Sprzedawca\").show()\n",
    "print(f'not bucketed end: {datetime.now()-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ea73456533203",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2.2 wykonaj partycjonowanie danych i zapisz je w formcie csv (wypróbuj partycjonowanie wg. kraju, nazwiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3405385cb0681a3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "output_path_kraj = \"zamowienia_partitioned_kraj\"\n",
    "\n",
    "df.write.partitionBy(\"Kraj\").mode('overwrite').saveAsTable(output_path_kraj)\n",
    "\n",
    "df.write.partitionBy(\"Kraj\").mode('overwrite').format(\"csv\").option(\"header\", \"true\").save(output_path_kraj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d01cb2083c2ec2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "output_path_nazwiska = \"zamowienia_partitioned_nazwiska\"\n",
    "\n",
    "df.write.partitionBy(\"Sprzedawca\").mode('overwrite').saveAsTable(output_path_nazwiska)\n",
    "\n",
    "df.write.partitionBy(\"Sprzedawca\").mode('overwrite').format(\"csv\").option(\"header\", \"true\").save(output_path_nazwiska)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fcce714ffe498a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2.3 wykonaj zapytanie agregujące z filtrowanie po kolumnie, której użyłeś/-aś do partycjonowania na danych oryginalnych oraz partycjonowanych i porównaj czas wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f57922a626a1d270",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|  Kraj|       avg(Utarg)|\n",
      "+------+-----------------+\n",
      "|Polska|1550.376319335228|\n",
      "+------+-----------------+\n",
      "\n",
      "not partitioned end: 0:00:00.345065\n",
      "+------+-----------------+\n",
      "|  Kraj|       avg(Utarg)|\n",
      "+------+-----------------+\n",
      "|Polska|1550.376319335228|\n",
      "+------+-----------------+\n",
      "\n",
      "partitioned end: 0:00:00.428861\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "# df.filter(df.Kraj == 'Polska').groupby('Kraj').agg({'Utarg': 'avg'}).show()\n",
    "spark.sql(\"select Kraj, avg(Utarg) from ZAMOWIENIA_DATA where Kraj='Polska' group by Kraj\").show()\n",
    "print(f'not partitioned end: {datetime.now()-start}')\n",
    "\n",
    "start = datetime.now()\n",
    "spark.sql(\"select Kraj, avg(Utarg) from zamowienia_partitioned_kraj where Kraj='Polska' group by Kraj\").show()\n",
    "print(f'partitioned end: {datetime.now()-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dfc8ff76d8336c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Zadanie 3 \n",
    "Z danych wygeneruj 4 różne podzbiory próbek (wiersze wybrane losowo) i dodaj nową kolumnę w każdym z nich, np. w jednym stwórz kolumnę month wyciągając tylko miesiąc z daty, w drugim wartość netto zamówienia (przyjmując, że vat to 23%), w kolejnym zamień nazwisko na wielkie litery, w kolejnym dodaj kolumnę waluta z wartością PLN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3a8c31ca578741a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, month, expr, upper, lit\n",
    "\n",
    "# Wygenerowanie czterech różnych podzbiorów danych z losowymi wierszami\n",
    "subset1 = df.sample(fraction=0.5, seed=42)\n",
    "subset2 = df.sample(fraction=0.5, seed=43)\n",
    "subset3 = df.sample(fraction=0.5, seed=44)\n",
    "subset4 = df.sample(fraction=0.5, seed=45)\n",
    "\n",
    "# Dodanie nowych kolumn w każdym podzbiorze\n",
    "subset1 = subset1.withColumn(\"Month\", month(col(\"Data zamowienia\")))\n",
    "\n",
    "subset2 = subset2.withColumn(\"Netto\", (col(\"Utarg\") / 1.23).alias(\"Netto\"))\n",
    "\n",
    "subset3 = subset3.withColumn(\"Sprzedawca 2\", upper(col(\"Sprzedawca\")))\n",
    "\n",
    "subset4 = subset4.withColumn(\"Waluta\", lit(\"PLN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d58afe3cc10e445",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Następnie zapisz każdy z tych zbiorów tak, że:\n",
    "- zbiór pierwszy to będzie tymczasowa tabela in-memory Sparka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae4168e323a3a5b4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "subset1.createOrReplaceTempView(\"subset1_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc1bd278e6038d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- zbiór drugi to plik(i) parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dfe1b447dcbc6bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "output_parquet = \"subset2_parquet\"\n",
    "subset2.write.mode(\"overwrite\").parquet(output_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e46b66bf774d55",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- zbiór trzeci to plik(i) csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdc22c2157446521",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "output_csv = \"subset3_csv\"\n",
    "subset3.write.mode(\"overwrite\").csv(output_csv, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7a0dc5a260de2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- zbiór czwarty to plik(i) json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77ce8c39fd1191f4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "output_json = \"subset4_json\"\n",
    "subset4.write.mode(\"overwrite\").json(output_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292b11b090ae8eb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wykonaj zapytanie złączające jak w przykładzie pobierając dane bezpośrednio z plików i wyświetl idZamowienia, Kraj, Sprzedawcę, Datę, Utarg oraz 4 nowo utworzone kolumny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a55b2241b83ad053",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/01 15:26:14 WARN ObjectStore: Failed to get database json, returning NoSuchObjectException\n",
      "24/12/01 15:26:14 WARN ObjectStore: Failed to get database parquet, returning NoSuchObjectException\n",
      "24/12/01 15:26:14 WARN ObjectStore: Failed to get database csv, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+---------------+-----+------+-------+------------------+------------+\n",
      "|   id|Sprzedawca|  Kraj|Data zamowienia|Month|Waluta|  Utarg|             Netto|Sprzedawca 2|\n",
      "+-----+----------+------+---------------+-----+------+-------+------------------+------------+\n",
      "|10274|  Sowiaski|Polska|     2003-08-16|    8|   PLN|  538.6| 437.8861590129573|    SOWIASKI|\n",
      "|10288|   Peacock|Niemcy|     2003-09-03|    9|   PLN|   80.1| 65.12194997896025|     PEACOCK|\n",
      "|10315|   Peacock|Niemcy|     2003-10-03|   10|   PLN|  516.8| 420.1625917016006|     PEACOCK|\n",
      "|10319|      King|Polska|     2003-10-11|   10|   PLN| 1191.2|  968.455244855183|        KING|\n",
      "|10321| Leverling|Niemcy|     2003-10-11|   10|   PLN|  144.0|117.07317073170732|   LEVERLING|\n",
      "|10330| Leverling|Niemcy|     2003-10-28|   10|   PLN| 1649.0|1340.6504065040651|   LEVERLING|\n",
      "|10337|   Peacock|Niemcy|     2003-10-29|   10|   PLN| 2467.0|2005.6910569105692|     PEACOCK|\n",
      "|10338|   Peacock|Niemcy|     2003-10-29|   10|   PLN|  934.5| 759.7560975609756|     PEACOCK|\n",
      "|10342|   Peacock|Niemcy|     2003-11-04|   11|   PLN|1840.64|1496.4552964621444|     PEACOCK|\n",
      "|10389|   Peacock|Niemcy|     2003-12-24|   12|   PLN| 1832.8|1490.0813405106708|     PEACOCK|\n",
      "+-----+----------+------+---------------+-----+------+-------+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT ed.id, ed.Sprzedawca, ed.Kraj, ed.`Data zamowienia`,ed.Month, json.Waluta, ed.Utarg, parquet.Netto, csv._c5 as `Sprzedawca 2`\n",
    "\n",
    "FROM json.`subset4_json/` as json \n",
    "join subset1_temp ed on json.id=ed.id\n",
    "join parquet.`subset2_parquet/` as parquet on ed.id=parquet.id\n",
    "join csv.`subset3_csv/` as csv on ed.id=csv._c3\n",
    "\"\"\"\n",
    "df_from_json = spark.sql(query).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ddb223e012a9df",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Zatrzymanie sesji Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
