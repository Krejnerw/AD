{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b7003a-d147-46f3-bfd9-2fb4bbacd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_NAME'] = \"/opt/spark\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "# os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/spark/work-dir/myenv/bin/python3'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/spark/work-dir/myenv/bin/python3'\n",
    "\n",
    "# można też spróbować wykorzystać moduł findspark do automatycznego odnalezienia miejsca instalacji sparka\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# lub\n",
    "# findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd70f81e-f1a3-4465-9b99-02f0059b7e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/16 16:49:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ce3216d7658c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create-DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Create-DataFrame>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName(\"Create-DataFrame\").getOrCreate()\n",
    "# konfiguracja z określeniem liczby wątków (2) oraz ilości pamięci do wykorzystania poza stertą interpretera Pythona\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Create-DataFrame\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca207a75-f38a-4334-a572-93f575b8c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c4b20-cbe0-4a4b-acda-4c1b14ca66b2",
   "metadata": {},
   "source": [
    "**Zadanie 1*  \n",
    "Na zbiorze danych '_Recipe Reviews ..._' wykonaj:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78c730ae-2e45-4ee5-a161-7ab70b05d51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- recipe_number: string (nullable = true)\n",
      " |-- recipe_code: string (nullable = true)\n",
      " |-- recipe_name: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_reputation: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- reply_count: string (nullable = true)\n",
      " |-- thumbs_up: string (nullable = true)\n",
      " |-- thumbs_down: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- best_score: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "df_reviews = spark.read.csv('./data/Recipe Reviews and User Feedback Dataset.csv', header=True, sep=\",\")\n",
    "df_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7de50-4623-4e95-aa94-cae011280035",
   "metadata": {},
   "source": [
    "  \n",
    "1.1  Zmień nazwę pierwszej kolumny z `_c0` na `id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5aa1387-503e-4b4c-9a60-e556987bf266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- recipe_number: string (nullable = true)\n",
      " |-- recipe_code: string (nullable = true)\n",
      " |-- recipe_name: string (nullable = true)\n",
      " |-- comment_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_reputation: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- reply_count: string (nullable = true)\n",
      " |-- thumbs_up: string (nullable = true)\n",
      " |-- thumbs_down: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- best_score: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews = df_reviews.withColumnRenamed('_c0','id')\n",
    "df_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091f3160-b694-4dbb-924f-4be50d0c0a43",
   "metadata": {},
   "source": [
    "1.2  Wyświetl 10 najwyższych wartości w kolumnie `reply_count`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d6b94e-e85e-4687-89ca-8cd83745f2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|reply_count|\n",
      "+-----------+\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          2|\n",
      "|          2|\n",
      "|          2|\n",
      "|          2|\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews.select(df_reviews.reply_count).sort(desc('reply_count')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018906bc-62fe-4a59-ba67-17e3fbc268b2",
   "metadata": {},
   "source": [
    "1.3  Wyświetl 10 najwyższych sum wartości w kolumnie `best_score` dla każdego przepisu (grupowanie).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9906e2-0500-4dad-a791-31def0390305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|recipe_code|sum(best_score)|\n",
      "+-----------+---------------+\n",
      "|       2832|        98863.0|\n",
      "|      14299|        85497.0|\n",
      "|      17826|        64880.0|\n",
      "|       3309|        64247.0|\n",
      "|      21444|        60755.0|\n",
      "|      32480|        59867.0|\n",
      "|      12540|        59195.0|\n",
      "|       2912|        54032.0|\n",
      "|      42083|        51975.0|\n",
      "|      19731|        47905.0|\n",
      "+-----------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews.groupby('recipe_code').agg({'best_score': 'sum'}).sort(desc('sum(best_score)')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d9e6f5-d3e5-4f9e-92f4-7de4d488fc87",
   "metadata": {},
   "source": [
    "1.4  Które 10 przepisów miało najwięcej komentarzy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1fc5c65-5bc8-4cd9-97fd-06fd5981f9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|recipe_code|count(comment_id)|\n",
      "+-----------+-----------------+\n",
      "|       2832|              725|\n",
      "|      14299|              654|\n",
      "|       3309|              509|\n",
      "|      42083|              421|\n",
      "|      32480|              397|\n",
      "|      21444|              395|\n",
      "|      12540|              368|\n",
      "|      17826|              338|\n",
      "|       2912|              332|\n",
      "|      19731|              324|\n",
      "+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews.groupby('recipe_code').agg({'comment_id': 'count'}).sort(desc('count(comment_id)')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf00692-b305-4daa-9c9e-cd2eacf10799",
   "metadata": {},
   "source": [
    "1.5  Wyświetl rozkład wartości w kolumnie `stars`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b33d52-8c9c-49cf-91ba-6e26c3fd48b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             stars|\n",
      "+-------+------------------+\n",
      "|  count|             18182|\n",
      "|   mean|  4.28880211197888|\n",
      "| stddev|1.5447863581965313|\n",
      "|    min|                 0|\n",
      "|    max|                 5|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reviews.select(df_reviews.stars).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72569f42-d7f3-4192-ab26-4d7d057da112",
   "metadata": {},
   "source": [
    "**Zadanie 2**  \n",
    "Wczytaj zbiór danych `employee` nakazując Sparkowi wywnioskowanie bardziej optymalnych typów danych niż domyślny typ `string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af7f16f-96c1-4386-b57b-118356d7e562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('employee.csv', header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183e9d5-7669-49ef-a7dc-569a0d5c3f5f",
   "metadata": {},
   "source": [
    "**Zadanie 3**  \n",
    "Jaki jest czas wykonania operacji `df.filter(df[\"salary\"] > 10000).count()` tym razem przy numerycznym typie kolumny `salary`? Jest jakaś różnica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "084cffbb-b32b-4f09-921d-87dc972afb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end: 0:00:14.515439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.read.csv('employee.csv', header=True)\n",
    "\n",
    "start = datetime.now()\n",
    "df.filter(df[\"salary\"] > 10000).count()\n",
    "print(f'end: {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9c5a1ed-8a40-4e02-982d-ac09ac7460bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- salary: decimal(10,2) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('employee.csv', header=True)\n",
    "\n",
    "df = df.withColumn(\"salary\", F.col(\"salary\").cast(\"decimal(10,2)\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462c1f30-cb2d-471b-8473-6adc6fc65f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end: 0:00:16.627322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "df.filter(df[\"salary\"] > 10000).count()\n",
    "print(f'end: {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6602f0-e039-4323-9a67-1e854d321c9d",
   "metadata": {},
   "source": [
    "**Zadanie 4**  \n",
    "Wykorzystując przykład z dokumentacji klasy `Bucketizer` (https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Bucketizer.html) podziel dane w kolumnie `age` zbioru `employee` na buckety co 10 lat (10-19, 20-29, ..., 60-69)\n",
    "wyświetl te dane dla 20 pierwszych wierszy w formie surowej \n",
    "oraz całość grupując po bucketach i licząc ile osób znalazło się w każdym z nich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11b47b6592e97971",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: decimal(10,2) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "df = df.withColumn(\"age\", F.col(\"age\").cast(\"integer\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc72225b4bebf5f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+---+-------+----------+\n",
      "| id| firstname|           lastname|age| salary|age_bucket|\n",
      "+---+----------+-------------------+---+-------+----------+\n",
      "|  1|   Wisława|             Wlotka| 33|8303.97|       2.0|\n",
      "|  2|     Agata|         Malinowski| 60|7960.70|       5.0|\n",
      "|  3|Aleksandra|Brzęczyszczykiewicz| 25|8966.46|       1.0|\n",
      "|  4|     Agata|               Glut| 55|7449.52|       4.0|\n",
      "|  5|     Agata|              Pysla| 18|7788.84|       0.0|\n",
      "|  6|     Marek|         Wróblewski| 26|8603.35|       1.0|\n",
      "|  7|   Wisława|           Barański| 27|6830.76|       1.0|\n",
      "|  8|     Marek|             Wlotka| 50|7255.81|       4.0|\n",
      "|  9| Krzysztof|              Pysla| 58|6616.81|       4.0|\n",
      "| 10|Aleksandra|         Malinowski| 60|6541.10|       5.0|\n",
      "| 11|Mieczysław|           Barański| 52|7524.52|       4.0|\n",
      "| 12| Katarzyna|              Pysla| 29|6680.68|       1.0|\n",
      "| 13|      Adam|         Malinowski| 25|8819.53|       1.0|\n",
      "| 14|Mieczysław|           Barański| 30|8574.76|       2.0|\n",
      "| 15|     Agata|       Mieczykowski| 65|8739.78|       5.0|\n",
      "| 16|     Marek|               Glut| 32|6136.22|       2.0|\n",
      "| 17|   Wisława|       Mieczykowski| 60|6535.21|       5.0|\n",
      "| 18| Katarzyna|       Mieczykowski| 47|8510.23|       3.0|\n",
      "| 19|      Adam|             Szczaw| 39|7246.52|       2.0|\n",
      "| 20|     Marek|           Barański| 25|8596.00|       1.0|\n",
      "+---+----------+-------------------+---+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = [10, 20, 30, 40, 50, 60, 70]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"age\", outputCol=\"age_bucket\")\n",
    "\n",
    "# Przekształcamy dane - dodajemy kolumnę z bucketami\n",
    "bucketed_df = bucketizer.transform(df)\n",
    "\n",
    "bucketed_df.select('*').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28be07c10c8b5136",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|age_bucket|  count|\n",
      "+----------+-------+\n",
      "|       0.0| 784973|\n",
      "|       1.0|3921202|\n",
      "|       2.0|3921730|\n",
      "|       3.0|3921517|\n",
      "|       4.0|3921697|\n",
      "|       5.0|3528881|\n",
      "+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "bucketed_counts = (\n",
    "    bucketed_df.groupBy(\"age_bucket\")\n",
    "    .count()\n",
    "    .orderBy(\"age_bucket\")\n",
    ")\n",
    "bucketed_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322234a-85d0-4b51-a025-087e97cdb2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
